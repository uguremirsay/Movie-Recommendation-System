# -*- coding: utf-8 -*-
"""MovieRecommendationSystem (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NHCQPWu1Yte4rDJfDGSxUAIy4pOl8lB_

---
"""

#importing required libraries

# Data processing
import pandas as pd
import numpy as np
import scipy.stats

# Visualization
import seaborn as sns

#importing datasets
movies = pd.read_csv("movies.csv")
ratings = pd.read_csv("ratings.csv")

#Getting dataset information
movies.head()

movies.info()

ratings.head()

ratings.info()

#To have a better understanding of the data we grab needed informations
print('The ratings dataset has:')
# Number of users
print('unique users: ', ratings['userId'].nunique())
# Number of movies
print('unique movies: ', ratings['movieId'].nunique())
# Number of ratings
print('unique ratings:', ratings['rating'].nunique())
# List of unique ratings
print('unique rating list of ratings dataset:', sorted(ratings['rating'].unique()))

# Merge ratings and movies datasets
df = pd.merge(ratings, movies, on='movieId', how='inner')
# Take a look at the data
df.head()

data = ratings.pivot(index='movieId',columns='userId',values='rating')

# To understand it better to get information about data
data.head()

#As seen above we need to fill NaN values with 0 to proceed further to have more accurate results for further implementations
data.fillna(0,inplace=True)

# To improve credibility of the system, I decided to reduce noise by filtering.

# Aggregation by movie
agg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),number_of_ratings = ('rating', 'count')).reset_index()

# over 150 ratings movies which we assume it is popular and make it possible for analyze
agg_ratings_bynum = agg_ratings[agg_ratings['number_of_ratings']>150]
agg_ratings_bynum.info()

# Understanding popular movies by sorting them
agg_ratings_bynum.sort_values(by='number_of_ratings', ascending=False).head()

#Above we can understand that average rating is around 4 to make it more clear we proceed with visualization for this we use seaborn library
sns.jointplot(x='mean_rating', y='number_of_ratings', data=agg_ratings_bynum)

# Merge data
df_150 = pd.merge(df, agg_ratings_bynum[['title']], on='title', how='inner')
df_150.info()

# To see how many users rated movies over then 150 ratings we use filtering
# Number of users
number_of_users = df_150['userId'].nunique()
print('Num of users are:',number_of_users )

#to see the users how many movies that they rated we use filtering
number_of_movies =  df_150['movieId'].nunique()
print('Num of movies are:', number_of_movies)

"""We can see clearly and conclude that there are **582** users which they rated **43** number of movies.

"""

# List of unique ratings
print('unique rating list of ratings dataset is:', sorted(df_150['rating'].unique()))

# Creating USER-ITEM matrix

#Here we selected rows as users and columns as movies
matrix = df_150.pivot_table(index='userId', columns='title', values='rating')
matrix.head()

# Now we start Data Normalization here.
# Normalizing user-item matrix
matrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')
matrix_norm.head()

"""After normalization, movies with ratings that are lower than the average user rating receive a *negative value*, and movies with ratings that are higher than the average user rating receive a *positive value*.

Below for KNN (second part of this code that we will show) we have used *Cosine Distance*. There are many ways for similarity calculation but the most used ones are Pearson Correlation and Cosine Similarity. For this part we will firstly follow *Pearson Correlation* as below:
"""

# User similarity matrix using Pearson correlation
user_similarity = matrix_norm.T.corr()
user_similarity.head()

# Pick a user ID
picked_userid = 1

# Remove picked user ID from the candidate list
user_similarity.drop(index=picked_userid, inplace=True)

# Take a look at the data
user_similarity.head()

# Number of similar users
n = 10

# User similarity threashold
user_similarity_threshold = 0.4

# Get top n similar users
similar_users = user_similarity[user_similarity[picked_userid]>user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]

# Print out top n similar users
print(f'The similar user list for user with id of {picked_userid} are', similar_users)

# Movies viewed by others and delete any films that none of your users have watched.

similar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all')
similar_user_movies

# A dictionary to store item scores
item_dict = {}

# Loop through items
for item in similar_user_movies.columns:

  # Get the ratings for movie item
  movierat_loop = similar_user_movies[item]

  # Create a total variable to store the score
  t = 0

  # Create a count variable to store the number of scores
  c = 0

  # Loop through similar users
  for user in similar_users.index:

    # If the movie has rating
    if pd.isna(movierat_loop[user]) == False:

      # Score is the sum of user similarity score multiply by the movie rating
      score = similar_users[user] * movierat_loop[user]

      # Add the score to the total score for the movie so far
      t += score
      # Add 1 to the count
      c +=1

  # Get the average score for the item
  item_dict[item] = t / c

# Convert dictionary to pandas dataframe
item_dict = pd.DataFrame(item_dict.items(), columns=['movie', 'movie_score'])

# Sort the movies by score
ranked_looped_score = item_dict.sort_values(by='movie_score', ascending=False)

# Select top f movies
f = 10
ranked_looped_score.head(f)

"""Knowing the rank of the things is sufficient if the objective is to select the suggested items. The average user movie rating score must be added back to the movie score if the objective is to anticipate the user's rating.


"""

# Average rating for the picked user
avg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]

# Print the average movie rating for user 1
print(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}')

noof_user_voted = ratings.groupby('movieId')['rating'].agg('count')
noof_movies_voted = ratings.groupby('userId')['rating'].agg('count')

data = data.loc[noof_user_voted[noof_user_voted > 10].index,:]
data= data.loc[:,noof_movies_voted[noof_movies_voted > 50].index]

from scipy.sparse import csr_matrix
csr_data = csr_matrix(data.values)
data.reset_index(inplace=True)

"""Because recommender systems use a lot of CPU and memory, sparsity is a concern for recommender systems. Using the *csr matrix function* from the Scipy library, I lessened the sparsity."""

from sklearn.neighbors import NearestNeighbors
knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)
knn.fit(csr_data)

def movie_recommendation(movie_name):
    # We do not wanted to interfere with numbers that is why decided on 10 recommendations as n.
    n = 10
    movieList = movies[movies['title'].str.contains(movie_name)]
    if len(movieList):
        movie_id= movieList.iloc[0]['movieId']
        movie_id = data[data['movieId'] == movie_id].index[0]

        distance , indice = knn.kneighbors(csr_data[movie_id],n_neighbors=n+1)
        movie_indices = sorted(list(zip(indice.squeeze().tolist(),distance.squeeze().tolist())),key=lambda x: x[1])[:0:-1]

        # Created list for recommendations
        frame = []
        for value in movie_indices:
            movie_id = data.iloc[value[0]]['movieId']
            idR = movies[movies['movieId'] == movie_id].index
            frame.append({'Title':movies.iloc[idR]['title'].values[0],'Distance':value[1]})
        result = pd.DataFrame(frame,index=range(1,n+1))
        return result
    else:
        return "Sorry,no movies found."

movie_recommendation('Star Trek')

# We choose as a team since we are fan of Star Trek to see which other movies match our taste of movie.

"""As a conclusion of this function we can see the top 10 similar films to our taste."""